《摘要》
    随着领先的在线旅游企业建立自己的旅行社和目的地接待体系，以及传统的旅行社逐渐的建立自己的网络平台，网络平台和旅行社呈现
相互融合的趋势。业内人士认为，集合了网络信息技术优势与旅行社专业服务的在线旅行社(Online Travel Agent, 简称OTA)，将是旅行电
子商务的主流和未来。由于在线旅行发展正值发展初期，为了抢夺市场，多家OTA之间的战争是越演激烈，尤其突出在抢夺酒店用户这一领域
上。每家OTA为了扩充自己的酒店数据，定期到竞争对手站点抓取酒店数据，然后通过酒店数据去重算法从抓取的酒店数据中筛选出新增的
酒店数据，并导入本地数据库。迫于扩展酒店数据的需求，本文实现了电商数据获取及分析系统。
    本文应用Django，Scrapy等框架，涉及KdTree，Shingling和LCS等算法，采用B/S结构构造系统构架，使用MySQL作为后台数据库，
以Django为应用服务器开发实现了电商数据获取及分析系统。设计开发的电商数据获取及分析系统共包含Spider抓取，数据展示，
导入新增酒店数据
三个模块。本文对电商数据获取及分析系统开发过程中的需求分析，概要设计，详细设计以及测试的各个阶段进行了较详细的说明和讨
论，并对Spider抓取模块和导入新增酒店数据模块进行了重点阐述。
    本文设计与实现的电商数据获取及分析系统使用了目前最流行的Python语言，基于Linux作为开发和运行平台，经过具体的编码实
现与系统测试，电商数据获取及分析系统中的各个模块均已成功实现，并已投入使用，且运行良好。

关键词：抓取酒店数据; Django; Scrapy; 去重算法

Abstract
// TODO

1.绪论
1.1 系统开发背景
    互联网技术的飞速发展，让人们的出行越来越便捷。过去，我们需要到火车站或代售点预定火车票，到航空公司或代售点预定机票，
到酒店预定房间，到旅行社团购旅行。如今，人们只需要通过Web或移动客户端就可以轻松实现网上订票和预定酒店，一场说走就走的旅
行从此不再是梦想。
    由于OTA集合了网络信息技术优势与旅行社的专业服务，极大地方便了人们的出行，所以越来越受到人们的青睐，市场的发展前景极佳。
2014年北京APEC会议提出了未来五大热门的行业，其中就包括在线旅行行业。有市场的地方就必然有竞争，由于在线旅游行业正值发展初
期，为了抢夺市场，多家OTA之间的战争是越演激烈，尤其突出在抢夺酒店用户这一领域上。每家OTA为了更好地发展自己的线上酒店产品，
让自己在酒店竞争中获得先机，所以扩充本地数据库中酒店数据的工作刻不容缓。
1.2 系统开发现状
    由于在线旅行行业正处于发展阶段，抢夺市场是各个OTA的首要目标。各个OTA在发展自己业务的同时，在条件的允许下会成立Spider
小组搭建Spider框架，负责抓取工作。
    由于涉及公司隐私，各个OTA使用的Spider技术和酒店数据分析的去重算法我们都无法具体得知，但是广泛使用的Spider框架无非以下几种：
    (1)PHP的Goutte框架
    (2)Java的Heritrix框架
    (3)C#的NWebCrawler爬虫框架
    (4)Python的Scrapy框架
    常用的字符串判重算法包括：
    (1)Shingling
    (2)Longest Common Subsequence
    (3)Jaccard
    (4)Simhash
    以上的Spider框架和字符串判重算法已经比较成熟，但是将Spider和Django框架结合使用的例子还是比较少见。
由于涉及公司隐私，对其余OTA的抓取和数据分析工作，我们无法获得比较详细的信息，不过在艺龙，酒店数据静态抓取的工作还从未开展过。
1.3 系统开发意义
    虽然前部分所描述的技术均为当下比较热门和常用的技术，但是充分利用且基于上述框架设计和开发的电商数据获取及分析系统仍比较少
见。虽然在艺龙有专业的Spider小组，但是他们隶属于搜索组，不会负责扩展酒店数据的抓取。所以，设计与开发电商数据获取及分析系统
有着非常重要的实际意义：
    (1)抢夺酒店市场，扩展自己的线上酒店产品，让自己在酒店竞争中获得先机。
    (2)填补酒店产品组静态抓取的空白
    (3)使用的技术Django，Scrapy以及酒店数据去重算法等技术十分前沿，使用Python作为主要开发语言，使得系统轻便简洁，容易维护，
易于扩展。结合使用KdTree，Shingling，LCS等算法，设计出了一种高效的酒店数据去重算法。
1.4 论文组织结构
    本文主要内容是电商数据获取及分析系统设计与实现。在文中对当今的互联网部分新技术进行了介绍与分析，选取Django，Scrapy作为框
架，通过Spider抓取和酒店数据去重算法对系统进行实现。完成的主要工作包括：Spider抓取，数据展示，酒店数据去重算法设计，服务器搭建，数据库设计等
工作，实现了酒店静态信息抓取及展示，发送抓取结果邮件，导入新增酒店数据到本地数据库等功能。
    论文的主要结构如下：
    第一章：绪论。介绍了课题的研究背景，现状以及意义。简介的分析了与课题相关的技术与发展趋势。
    第二章：电商数据获取及分析系统相关技术介绍。详细介绍了可用于项目开发的相关技术以及特点。
    第三章：电商数据获取及分析系统需求分析，根据用户对系统进行功能，业务用例，数据流图，非功能性需求分析等方面的设计。
    第四章：电商数据获取及分析系统的系统设计。对系统进行详细的分析与设计，包括系统总体架构，数据库设计，Spider设计，
去重算法设计，日志模块设计等进行了明确的定义和说明，同时进行了数据模型和系统中类，属性的定义。
    第五章：电商数据获取及分析系统的系统实现。对系统的搭建环境进行了说明，并对Spider抓取流程和去重算法进行了分析。
    第六章：系统测试。通过日志模块和测试方案明确测试用例，并且对测试的结果进行分析。

2.相关技术
    本章将详细介绍系统设计需要的核心技术，通过对技术的介绍与分析为后文实际开发提供坚实的理论支持。
2.1 B/S结构与C/S架构
    B/S结构即众所周知的Browser/Server结构，客户端使用浏览器（Browser）进行浏览，浏览器负责与服务器端的交互工作。
由于浏览器的标准化工作相对完善，因此客户端的压力比较小，核心的逻辑业务都由服务器端处理。这种模式使用统一的客户端，简化了
开发、使用以及维护的过程。由于现在任何操作系统都配有浏览器，B/S的普适性也有很大的优势。
    相较于传统的C/S（Client/Server）结构， B/S结构有很多优点，其分布性强，便于共享，便于扩展，兼容性强，容易快速安装配置。
B/S与C/S更为深入的比较可以如表xx.xx所示：
    表xx.xx
    B/S本质上是C/S的一种特殊表现形式，是对C/S这种模式的一种改进。对于本文使用的技术而言，B/S对于实现酒店静态信息展示的功能
更有优势。

2.2 Django Web开发框架
    本小节主要介绍用于开发的Web框架Django。从当前最流行的Python语言，MVC设计模式及Django的开发框架三个方面进行介绍。
2.2.1 Python语言
    Python是一种面向对象的解释性的应用程序开发语言。Python语言简洁，清晰，语法简单，可用类库强大，非常适合程序开发人员进行
快速开发工作。
    就Python语言的主要特点而言，由于它本身是开源项目，因此与其他著名的开源项目有着类似的特点。具体来说，它的优点大致有以下
方面：
    移植性强：开源的本质决定了有大量的开发者将Python移植到多种平台之上。主流的操作系统如Linux，Macintosh，Windows都已经支持
Python语言。
    介于高层的编程语言：Python良好的封装与设计使得开发者无需考虑程序本身如何操作内存等细节。
    扩展性，嵌入型强：由于Python良好的兼容性，可以利用C/C++等程序语言，然后在Python程序中使用。
    丰富的资源库：Python标准类库涵盖到方方面面。可以处理数据库，网页浏览器，正则表达式等许多与系统相关的工作。不仅如此，社区
还提供了很多标准类库以外的库文件。
    面向对象：Python同时支持面向对象与面向过程编程。开发者可以利用数据功能的组合，对象以及对象的属性操作来对程序进行操作控制。
    强缩进控制模式：Python使用强缩进控制的方式，使用户可以清晰地读懂代码结构，极大地提高了开发和维护效率。
    Python拥有“明确，优雅，简单”的设计哲学。由于Python的种种优点，让Python飙升为目前最流行的编程语言，被很多国内外机构广泛使用，
包括Google，Baidu，豆瓣，知乎等等。这些知名公司对Python的青睐，足以证明Python的强大和高效。
2.2.2 MVC设计模式
    Django使用的是MVC设计模式(Model-View-Controller)。这种开发模式迫使应用程序的输入，处理过程和输出结果分离。使得三个核心
部件各自处理自己的任务。下面简单介绍一下以上三个部件：
    模型也称为数据模型，他的功能是封装需要处理的逻辑业务相关的数据，并定义处理数据的方法。模型有着访问数据，操作数据库的权限。
    视图组件是用来显示数据，通常根据数据显示的定义规定处理的逻辑。
    控制器组件是用来组织不同层面，控制应用程序处理的流程。它对用户的行为，数据模型的转变进行处理，用合理的方式来响应请求。
    图xx.xx描述了MVC的基本工作模式。
    MVC的工作模式对于开发者来说有着很多优点：
    (1)允许多个视图共享一个数据模型。对于程序设计，同一个应用程序通常会有不同的用户界面，用户可用通过浏览器来进行浏览，也
可以通过移动设备来访问。上述问题就要求Web网站可以同时提供Internet和移动设备两种视图。MVC设计模式中，视图与模型分离，模型
响应用户的请求并且返回数据，视图组件负责呈现给用户外部显示。那么，业务逻辑和外在视图就分离了，提高了代码的重用性。
    (2)由于控制器的独立，应用程序的业务逻辑和数据源实现了低耦合。例如系统打算更换数据源，我们可以通过简单的更改控制器来实现
这一目的。由于三个模块之间的独立，我们对于控制器的更改，不会影响数据模型和视图层。
    (3)部署快，可维护性高，有利于工程化软件的管理等突出优点。
2.2.3 Django
    Django是一个开源的Web应用框架，整个Django框架是以MVC的核心思想设计而成。Django的设计目标就是简化复杂的，数据库驱动的网站
开发。Django自带丰富的开发组件可以使得开发人员以最少的代码，最快的速度，最方便的模式来进行各种Web应用程序的开发与调试。图xx.xx
便是Django的架构总览：
    Django核心框架包括：用于数据模型和关联数据库间交互的面向对象的解释器，基于正则表达式的URL派发器，用于处理请求、显示的
视图系统和一个模板系统。除此之外，Django还带有一个轻量级的Web服务器用于测试和开发，队请求处理各个阶段进行干涉的中间件，
还有可以读取特殊类型XML或者JSON的序列化系统以及一个用于扩展模板引擎的系统。
    在核心框架中，Django实现了MVC的设计理念。在实现MVC架构中，最关键的部分在于要正确的分割应用程序的不同层次。在Django对于
MVC框架的分层中，改变了一些做法。首先在模型部分，Django缩小了模型部分的定义，Django框架中的模型只负责把数据传入数据库。
对于视图部分而言，Django的视图并不单单具有显示数据的唯一功能，它的功能还带有MVC中控制器的一部分。Django开发团队曾说：
“在我们理解的MVC里，视图的作用是对要展示给用户的数据进行描述，这种描述不但包括数据的外观，还包括数据的是应该如何被表现的。”
也就是说，Django的表示曾丽，视图模型里定义了要对模板中的什么数据进行显示，而模板则定义了最终数据的显示方式。Django框架本身
决定了视图与模板之间的交互，以及相应指定请求的机制。框架本身更复合MVC中控制器的功能。
    在Django的核心框架中，与开发者最为相关的三个模块分别是模型、模板和视图。
    模型：所有应用程序的基础，应用程序中，模型是最底部的一层。作为基础，模型通常都十分稳定，不同于视图和模板等可以用不同的
表现形式替换。
    模板：绝大多数Django开发者们习惯使用模板来渲染HTML页面，模板就是经过特殊格式化的可以输出动态HTML的文本。除此之外，模板
还支持逻辑结构与循环。视图可以通过制定模板的方式，来显示所需要显示的信息，而且还可以使用到模板中渲染的结果。
    视图：视图总是覆盖Django应用程序里几乎全部的逻辑。通常，视图的定义是链接到一个或者多个URL上的Python函数，用Http响应对象
的方式返回。视图通常可以从模型里取出单个或者一列对象并将其显示，视图也可以向模型里添加这样的新对象。这是视图的主要功能。
在Django中，你既可以自己编写一些函数来控制这个过程，也可以使用框架本身提供的帮主函数来进行开发。在视图这一层面上，Django的
灵活性往往得到了充分的体现。
    在Django框架中，MVC模式有时也被称为“MTV”，即上文所述的模型（Model），模板（Template），视图（View）。整个工作流程大致如下
，受到的Http Requirement被Web服务器转发给Django框架，框架的中间件对他们接收。然后，URLconfig文件对他们分配，将其转入合理的
视图，视图通过模板或者是模型生成相应的需求响应。在经过另外一层中间件，将HTTP的相应转发给Web服务器随即转给用户。

2.3 Scrapy Spider开发框架
    本小节主要介绍Spider开发框架Scrapy。
    如图xx.xx，显示了Scrapy的大体架构，其中包含了它的主要组件及系统的数据处理流程。
2.3.1 Scrapy组件
Scrapy Engine(Scrapy引擎)：
    Scrapy引擎是用来控制整个系统的数据处理流程，并进行事物处理的触发。
Scheduler(调度)：
    调度程序从Scrapy引擎接受请求并排序列入队列，并在Scrapy引擎发出请求后返还给他们。
Downloader(下载器)：
    下载器的主要职责是抓取网页并将网页内容返还给Spiders
Spider(蜘蛛)：
    Spiders是Scrapy用户自定义用来解析网页并抓取指定URL返回的内容的类，每个Spider都能处理一个域名或一组域名。Spider
就是用来定义特定网站的抓取和解析规则。
    Spider的整个抓取流程如下：
    1.首先获取第一个URL的初始请求，当请求返回后调取一个回调函数。第一个请求是通过调用start_requests()方法。该方法默认从start_urls中的Url中生成请求，并执行解析来调用回调函数。
    2.在回调函数中，你可以解析网页响应并返回项目对象和请求对象或两者的迭代。这些请求也将包含一个回调，然后被Scrapy下载，然后有指定的回调处理。
    3.在回调函数中，你解析网站的内容，同程使用的是Xpath选择器（但是你也可以使用BeautifuSoup, lxml或其他任何你喜欢的程序），并生成解析的数据项。
    4.最后，从蜘蛛返回的项目通常会进驻到项目管道。
Item Pipeline(项目管道)：
    项目管道的主要责任是负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。每个项目管道的组件都是有一个简单的方法组成的Python类。他们获取了项目并执行他们的方法，同时他们还需要确定的是是否需要在项目管道中继续执行下一步或是直接丢弃掉不处理。
    项目管道通常执行的过程有：
    1.清洗HTML数据
    2.验证解析到的数据（检查项目是否包含必要的字段）
    3.检查是否是重复数据（如果重复就删除）
    4.将解析到的数据存储到数据库中
Downloader middlewares(下载器中间件):
    下载中间件是位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。它提供了一个自定义的
代码的方式来拓展 Scrapy的功能。下载中间器是一个处理请求和响应的钩子框架。他是轻量级的，对Scrapy尽享全局控制的底层的系统。
Spider middlewares(蜘蛛中间件):
    蜘蛛中间件是介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。它提供一个自定义代码的方式来拓展
Scrapy的功能。蛛中间件是一个挂接到Scrapy的蜘蛛处理机制的框架，你可以插入自定义的代码来处理发送给蜘蛛的请求和返回蜘蛛获取的
响应内容和项目。
Scheduler middlewares(调度中间件):
    调度中间件是介于Scrapy引擎和调度之间的中间件，主要工作是处从Scrapy引擎发送到调度的请求和响应。他提供了一个自定义的代码
来拓展Scrapy的功能。

2.3.2 Scrapy数据处理流程
Scrapy的整个数据处理流程由Scrapy引擎进行控制，其主要的运行方式为：
1.引擎打开一个域名，时蜘蛛处理这个域名，并让蜘蛛获取第一个爬取的URL。
2.引擎从蜘蛛那获取第一个需要爬取的URL，然后作为请求在调度中进行调度。
3.引擎从调度那获取接下来进行爬取的页面。
4.调度将下一个爬取的URL返回给引擎，引擎将他们通过下载中间件发送到下载器。
5.当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎。
6.引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。
7.蜘蛛处理响应并返回爬取到的项目，然后给引擎发送新的请求。
8.引擎将抓取到的项目项目管道，并向调度发送请求。
9.系统重复第二部后面的操作，直到调度中没有请求，然后断开引擎与域之间的联系。



2.4 去重算法
    对于不同的OTA，他们对同一个实体酒店的存储信息很有可能不同，这种不同体现在酒店名称，酒店电话等等。尽管同一个物理酒店的
静态信息可能不同，但是同一个实体酒店的信息在不同的OTA中总会极度相似，这便是去重算法的依据。
    由于我们已经不需要导入已有的实体酒店数据到本地数据库，所以，我们需要做酒店数据去重的模块，而酒店数据去重模块的核心就是去重
算法。
    本小节主要介绍酒店数据去重算法，包括KdTree，Shingling及LCS算法。
2.4.1 KdTree
    KdTree(K-dimensional Tree)，是一种高维索引二叉树形数据结构，常用于大规模的高维数据空间进行最近邻查找(Nearest Neighbor)和
和近似最近邻查找(Approximate Nearest Neighbor)。
KdTree性质：
    1.KdTree是一棵二叉树
    2.数据只存储在叶子节点
    3.非叶子节点存储的数据结构为(k,m)，k为划分左右子树的维度，m为维度值
    4.对于任意一个非叶子节点，将当前集合按照维度k和维度值m划分为集合left-tree和right-tree，使得任意x属于left-tree满足
x(k)<m，任意y属于right-tree满足y(k)>=m

KdTree构建算法：
    1. 在k维数据集合中选择具有最大方差的维度k，然后在该维度上选择中值m对该数据集合进行划分，得到两个子集合。同时创建
一个结点(k,m)，用于存储维度k和中值m。
    2.对两个子集合重复1步骤的过程，直至所有子集合都不能再划分为止。如果某个子集合不能再划分时，则将该子集合中的数据保存到
叶子节点。

KdTree最近邻查找的算法：
    设查询节点为Q
    1.从根节点开始依次与每个非叶子节点作比较， 若Q(k)<m，则访问左子树，否则访问右子树。当到达叶子节点L，计算dcur=dis(Q,L)，
pcur=L
    2. 进行回溯操作，若对任意一个分支而言，满足条件|Q(k)-m|<dcur且未被访问过，则该分支里可能存在叶子节点L，L满足条件
dis(Q,L)<dcur。若寻找到叶子节点L(dis(Q,L)<dcur)，则更新dcur=dis(Q,L)，pcur=L。
回溯的判断过程是从下往上进行的，直到回溯到根结点时已经不存在与P更近的分支为止。

KdTree在本文作用：
    由KdTree的性质和酒店的经纬度坐标，我们可以使用KdTree这种数据结构，通过酒店经纬度坐标完成某个酒店邻近的N个酒店的查找。
然后将这N个邻近酒店依次与目标酒店做近似比较。

2.4.2 Shingling
    Shingling算法常用于计算两个字符串的相似度。
    维基百科用一个浅显的例子讲解了shingling算法的原理。比如，一个字符串:"a rose is a rose is a rose"
    分词后的词汇(token，语汇单元)集合是
    (a,rose,is,a,rose,is, a, rose)
    那么w=4的4-shingling就是集合:
    {(a,rose,is,a), (rose,is,a,rose), (is,a,rose,is), (a,rose,is,a), (rose,is,a,rose)}
    去掉重复的子集合：
    {(a,rose,is,a), (rose,is,a,rose), (is,a,rose,is)}
    给定shingle的大小,两个字符串A和B的相似度 r 定义为:
    r(A,B)=|S(A)∩S(B)| / |S(A)∪S(B)|
    其中|A|表示集合A的大小。
    因此,相似度是介于0和1之间的一个数值，且r(A,A)=1,即一个文档和它自身 100%相似。
2.4.3 LCS
    LCS(Longest Common Subsequence)，寻找两个字符串最大公共子序列的算法。可以通过计算len(lcs)/max(len(str1),len(str2))来计算
字符串str1与str2的相似度。
    LCS类问题属于动态规划(dynamic programming)，通过组合子问题的解而解决整个问题。
    1.描述一个LCS
        给定一个序列X=<x1,x2,...,xm>，对i=0,1,...,m，定义X的第i个前缀为Xi=<x1,x2,...,xi>。例如，如果X=<A,B,C,B,D,A,B>，
    则X3=<A,B,C>
    设X=<x1,x2,...,xm>和Y=<y1,y2,...,yn>为两个序列，并设Z=<z1,z2,...,zk>为X和Y的任意一个LCS
    a.如果xm=yn，那么zk=xm=yn而且Zk-1是Xm-1和Yn-1的一个LCS
    b.如果xm！=yn，那么zk！=xm蕴含Z是Xm-1和Y的一个LCS
    c.如果xm！=yn，那么zk！=yn蕴含Z是X和Yn-1的一个LCS
    2. LCS递归解
    参照LCS的最优子结构，我们可以得到LCS递归公式：
    //TODO P210 15.14
    3. 计算LCS长度
    参照LCS递归解，我们可以计算出两个序列的LCS的长度。len(LCS)=c[m][n]

3.需求分析
    上一章介绍了电商数据获取及分析系统使用的技术，本章将对系统需求进行分析。
3.1 功能需求分析
    根据服务器端和用户端的需求，电商数据获取及分析系统需要实现以下几种功能：
    服务器端：
    (1)数据库的设计与实现
    (2)Spider抓取的设计与实现
    (3)导入新增酒店数据的设计与实现
    用户端：
    (1)用户登陆，用户管理，权限管理
    (2)Spider抓取数据展示
    (3)Spider参数配置
3.1.1 数据库的设计与实现
    使用关系型数据库MySQL，存储Spider数据和Spider参数。
    由于整个系统的核心是Spider抓取和导入新增酒店数据部分，业务层的需求相对较少。在数据库设计当中，尽量减少外健的使用，维持数据库
结构的简单性。
3.1.2 Spider抓取的设计与实现
    服务器端最核心的功能，若没有Spider抓取得到的数据，数据展示及导入新增酒店数据模块的功能都没有作用。
    对于Spider抓取的设计，需要分析站点的访问顺序和网页结构，然后设计出一套可行的抓取方案。然后在Spider抓取设计的基础上，
考虑抓取过程中可能遇到的异常并提供解决方案。
3.1.3 导入新增酒店数据的设计与实现
    服务器端重要性仅次于Spider抓取的核心功能。从Spider抓取的酒店数据中筛选出新增的酒店数据，导入到本地系统。
    导入新增酒店数据模块的核心为酒店去重算法。通过结合使用KdTree，
Shingling及LCS算法，设计出酒店去重算法。然后结合本地系统数据和Spider抓取数据，实现导入新增酒店数据模块的功能。
3.1.4 用户登陆，用户管理，权限管理
    在web前端，进入系统的用户需要事先登陆，且每一个用户都有一定的权限。如果用户有足够的权限，就能实现用户管理和权限管理。
3.1.5 Spider抓取数据展示
    成功登陆系统的用户，能查看权限范围内的Spider数据
3.1.6 Spider参数配置
    成功登陆系统的用户，能查看和修改权限范围内的Spider参数配置

3.2 业务用例设计
3.3 数据流图设计

4 系统设计
4.1 系统概要设计
4.1.1 系统总体架构
4.1.2 系统交互设计

4.2 系统详细设计
4.2.1 用户功能模块详细设计
4.2.2 数据库设计
    在MVC的框架当中，数据库的设计十分重要，直接会影响到数据对象以及对数据模型的处理等操作。数据库的设计要对系统的物理设计
和逻辑设计都有很清晰的情况下才能根据所选择的数据库类型制定准确的存储数据。
    数据库的设计首先要根据系统的总体设计结果，参照系统的数据流设计，建立一个正确的数据模型。这个数据模型应当可以准确反应用户
的需求，数据如果定义的不妥当，会影响到系统的扩展性，维护性，甚至影响系统的运行效率。数据库的设计一般参照以下基本原则[引用]：
    数据的完整性：数据的完整性是指数据的准确性，和依赖关系的稳定性。这两方面的稳定有助于保证输入的错误信息被识别。也防止将一
些无效操作、或者数据输入影响系统安全。数据完整性一般包括实体完整性、参照完整性、用户自定义完整性、域完整性等。
    数据的一致性：数据的一致性主要是指当数据库事物操作完成时，必须保证所有的数据都是一致的状态。数据必须保持一致性，如果数据
不一致，很有可能出现无用对象，冗余数据，导致系统有些功能无法实现的现象。在非关系型数据库中，数据的一致性更为重要，如果数据不
保持一致性，可能会导致数据之间识别错误。例如，不同表之间可能有相同的属性，如果这几个属性之间的格式不同。不但会影响到程序逻辑
，甚至可能影响到很多其他结果。
    数据命名规范性：数据命名规范性对于数据库的设计而言也非常关键，由于数据模型通常都是直接反应在对象设置，变量设置上。命名如
果不规范，不但不利于程序的编写，而且还会对编程逻辑产生不良的影响。相反，清晰的命名规则会方便开发人员的代码编写，而且也会帮助
数据库管理人员理清数据库表与表之间，字段属性间的物理逻辑关系。
    根据需求分析以及酒店数据模型基础，本电商数据获取及分析系统一共设计25张表。按照功能分类，可以分为：用户管理表，
Spider抓取数据表，Spider爬虫参数配置表。
    由于表的数量比较多，以下只列出几张核心的表的详细信息。
    auth_user
    auth_permission
    auth_user_user_permissions
    ctrip_city
    ctrip_hotel
    ctrip_hotelimage
    ctrip_hotelreview
    scrapy_manager_sendemailitem

4.2.3 Spider抓取详细设计
4.2.4 导入新增酒店数据详细设计

5 系统实现
5.1 系统环境搭建
5.1.1 系统环境综述
    开发平台上本系统选用了Ubuntu14.04作为开发的操作系统，在生产环境选用CentOS5.6作为运行的操作系统。
    开发环境选用的是Python2.7，Django1.6.8，Scrapy0.24.4，MysqlServer5.6。其中Python2.7是Ubuntu14.04系统中含有的，其他程序都需要
自己安装。
5.1.2 MysqlServer开发环境搭建
    数据库是开发系统的核心，在这里，我们选用开源且使用范围较广的MySQL。
    安装MySQL：
    sudo apt-get install mysql-server-5.6
    由于MySQL默认编码方式不为UTF-8，为了让MySQL良好的支持中文，我们配置MySQL的字符集：
    locate my.cnf
    找到my.cnf文件之后，打开该文件，在[client]下添加行default-character-set=utf8，在[mysqld]下添加行character-set-server=utf8。
    sudo /etc/init.d/mysql restart重启MySQL服务器。
    mysql -u username -p password 进入MySQL客户端，执行show variables like 'character%';
    若结果如图xx.xx所示，则表示将MySQL字符集设置为UTF-8成功。
    图xx.xx

5.1.3 Django Web 2.0开发环境搭建
    安装Django以及用Django搭建Web应用十分简单。
    安装Django只需要简单的执行以下几行命令：
    tar -zxvf Django-1.6.8.tar.gz
    cd Django-1.6.8
    sudo python setup.py
    等待命令执行完毕之后，即可完成对Django1.6.8的安装。然后执行以下步骤校验Django是否安装成功。如图xx.xx
    图xx.xx
    搭建Django Web应用程序也比较简单，一般来说开发一个Django Web应用的步骤大致如图xx.xx所示。
    图xx.xx
    首先会用django-admin.py文件创建一个Django Web工程，然后在工程目录下创建相应的应用。
    创建电商数据获取及分析系统的命令如下所示：
    django-admin.py startproject scrapy_project
    cd scrapy_project
    python manage.py startapp app
    python manage.py startapp common
    cd app
    python ../manage.py startapp elong
    python ../manage.py startapp ctrip
    python ../manage.py startapp scrapy_manager

     然后更改项目中的settings.py文件的配置，在文件中配置database的参数，应用等对应的关系。
     随后就是开发者根据数据模型开发models.py文件，按照数据库和类图的定义在此文件中定义数据模型和一些操作数据模型的方法。定义
好数据模型之后，在views.py文件中定义如何操作models.py中的数据对象以及接口的实现。然后通过编写前端展示views.py文件中接口返回
的数据。最后的流程是添加URL映射，这个映射相当于指定那些页面调用那些视图方法。
    以上是一个Django Web开发完成的流程，不过电商数据获取及分析系统的核心是Spider抓取和数据分析，使用Django Web只是为了方便
管理和展示静态数据。所以，为了简便开发过程，开发者使用Django框架自带的Admin模块来管理数据模型。因此在该系统中，
开发者在Django Web部分只需要关注models.py和admin.py文件。通过在admin.py文件中配置models.py文件中数据模型，即可在
Django Web Admin模块中展示和修改models.py文件中的数据对象，极大地提高了系统开发效率。

5.1.4 Scrapy开发环境搭建
    从Scrapy的官方文档中，我们可以获取简易的Scrapy安装方式。
    1.把Scrapy签名的GPG密钥添加到APT的钥匙环中:
        sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 627220E7
    2.执行如下命令，创建 /etc/apt/sources.list.d/scrapy.list 文件:
        echo 'deb http://archive.scrapy.org/ubuntu scrapy main' | sudo tee /etc/apt/sources.list.d/scrapy.list
    3.更新包列表并安装 scrapy-0.24:
        sudo apt-get update && sudo apt-get install scrapy-0.24
     通过以上步骤完成对Scrapy0.24的安装，由于电商数据获取及分析系统抓取了图片，所以我们还需要安装图片库。
     sudo apt-get install libjpeg-dev libfreetype6-dev zlib1g-dev
     sudo pip install Pillow

     创建好Django Web工程及搭建好Scrapy开发环境之后，我们接下来就将Scrapy工程嵌入到Django Web工程中。
     cd scrapy_project
     mkdir scrapys && cd scrapys
     mkdir ctrip && cd ctrip
     scrapy startproject hotel && cd hotel/hotel
     这样，我们就创建了一个抓取酒店静态数据的Spider，然后我们需要更改当前目录下的settings.py文件，配置Spider的环境变零，
访问网页时的DEFAULT_REQUEST_HEADERS和USER_AGENT，每两次抓取间隔时间DOWNLOAD_DELAY等等配置。
    其中配置Spider环境变量比较重要，通过配置Spider环境变量，使得Spider与Django框架融合。
    该配置如下：
    import os
    import sys
    sys.path.append("../../../../")
    os.environ['DJANGO_SETTINGS_MODULE'] = 'scrapy_project.settings'
    通过以上步骤，我们基本完成Scrapy开发环境的搭建。然后，我们需要在items.py文件中引用在Django的models.py文件中定义的数据
模型或者创建数据模型，在spider目录下创建Spiders完成数据抓取以及解析，在pipelines.py文件中对抓取及解析获得的item进行处理。

5.2 Spider抓取实现
    对电商数据获取及分析系统而言，若没有可以用来分析的数据，则无法完成后续的分析流程，所以Spider抓取模块在整个系统中占领
十分重要的地位。接下来，我们在基于Spider抓取设计的基础上对Spider抓取实现做详细说明。
    参照Spider抓取设计，我们已经得出一个可行的抓取方案，具体详情请参考Spider抓取设计部分。
5.2.1
5.2.2
5.2.3
5.3 去重算法实现
5.3.1
5.3.2
5.3.3

6 系统测试
